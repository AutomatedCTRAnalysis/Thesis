{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb32838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # matrix construction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "from ast import literal_eval\n",
    "\n",
    "import sklearn.metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a9978e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtagger\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/lgf21/anaconda3/lib/python3.9/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser','tagger', 'parser', 'ner']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38909ea4",
   "metadata": {},
   "source": [
    "# Opening Files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Json:\n",
    "# reading json: \n",
    "\n",
    "dataset1 = \"dataset_full_text.json\"\n",
    "path_data1 =  \"../src/scraping/dataset_full_text.json\"\n",
    "\n",
    "dataset2 = \"dataset_malware_text.json\"\n",
    "path_data2 =  \"../src/scraping/dataset_malware_text.json\"\n",
    "\n",
    "with open(path_data1) as file:\n",
    "    open_data = json.load(file)\n",
    "\n",
    "with open(path_data2) as file: \n",
    "    open_data2 = json.load(file)\n",
    "\n",
    "# Converting to Data Frames: \n",
    "    \n",
    "df1 = pd.DataFrame(open_data).transpose().reset_index(drop = True)\n",
    "\n",
    "df2 = pd.DataFrame(open_data2).transpose().reset_index(drop = False)\n",
    "df2.columns = ['url', 'mitre_domain', 'tech_name', 'tech_id', 'software_id', 'text'] # renaming columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72373b4a",
   "metadata": {},
   "source": [
    "# Merging our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis = 0)  # Create one Data Frame with both dataset1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabae64a",
   "metadata": {},
   "source": [
    "# Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76764176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning NAs in text: \n",
    "\n",
    "df['tactic_name'] = df['tactic_name'].fillna(\"\").apply(list) \n",
    "df['software_id'] = df['software_id'].fillna(\"\").apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'] != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cfd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning duplicates: \n",
    "\n",
    "dup = df[df.duplicated(subset='text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup = df.drop_duplicates(subset='text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup[df_no_dup['text'].isin(dup['text'])].sort_values(by='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a68a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dup.sort_values(by='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in dup.iterrows():\n",
    "    row_id = df_no_dup[df_no_dup['text'] == row['text']].index[0]\n",
    "    for col in ['mitre_domain', 'tech_id', 'tech_name', 'software_id', 'tactic_name']:\n",
    "        merged_list = df_no_dup.loc[row_id, col]\n",
    "        for item in row[col]:\n",
    "            if item not in merged_list:\n",
    "                merged_list.append(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup[df_no_dup['text'].isin(dup['text'])].sort_values(by='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_dup[df_no_dup['text'].isin(dup['text'])].sort_values(by='text').iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_dup[df_no_dup['text'].isin(dup['text'])].sort_values(by='text').iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446d700",
   "metadata": {},
   "source": [
    "# Filetering URLS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_no_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020fd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_url_relevant(url):\n",
    "    for word in ['microsoft', 'apple', 'github', 'wikipedia',\n",
    "                 'support.office', 'amazon', 'gitlab', 'capec', 'docker', 'youtube', 'google', 'mitre', 'zip', \n",
    "                 'twitter']:\n",
    "        if word in url:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df = df[df['url'].apply(is_url_relevant)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef1c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print('--------------')\n",
    "    print(row['url'])\n",
    "    print('--------------')\n",
    "    print(row['text'])\n",
    "    if i > 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv: \n",
    "\n",
    "df.to_csv(\"../src/merged_dataset.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2abb8",
   "metadata": {},
   "source": [
    "# Formatting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c81b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tech = df.explode(['tech_id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b49b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_concat.groupby(['tech_name']).mean().reset_index()\n",
    "tech_name = df_agg['tech_name'].unique()[0] # get first tech name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.loc[df_agg['tech_name'] == tech_name].iloc[:,1:].transpose().sort_values(by = 0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fadb0",
   "metadata": {},
   "source": [
    "# Add Tactic to dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b409380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_dataset_noMalwareNames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5f07b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['mitre_domain', 'tech_name', 'tech_id', 'software_id']:\n",
    "    df[col] = df[col].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773887fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tactic_dataset = \"tactic_dataset.json\"\n",
    "path_dataset = \"../src/tactic_dataset.json\"\n",
    "\n",
    "with open(path_dataset) as file: \n",
    "    open_data = json.load(file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tactic_list(tech_id):\n",
    "    tactics = []\n",
    "    tech_id_set = set(tech_id)\n",
    "    for tactic_id in open_data:\n",
    "        if len(tech_id_set.intersection(open_data[tactic_id]['Technique_ID'][0])) > 0:\n",
    "            tactics.append(tactic_id)\n",
    "    return tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tactic_id'] = df['tech_id'].apply(tactic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595881d",
   "metadata": {},
   "source": [
    "# Export Cleaned Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export new dataset for training: \n",
    "\n",
    "df.to_csv('training_dataset_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(open_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
